{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "from os.path import join\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20,7))\n",
    "    ax1.plot(history.history['acc'])\n",
    "    ax1.plot(history.history['val_acc'])\n",
    "    ax1.set_title('model accuracy')\n",
    "    ax1.set_ylabel('accuracy')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    \n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_title('model loss')\n",
    "    ax2.set_ylabel('loss')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(545, 535, 3) исходное\n",
    "IMG_SIZE = (197,197,3) #REznet FIGSIZE\n",
    "#IMG_SIZE = (224,224,3)\n",
    "# IMG_SIZE = (125,125,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# img = cv2.imread('./data/all_ica_img/'+file_names[117])\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "# plt.imshow(img[25:570, 185:720])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# img = cv2.imread('./data/all_ica_img/'+file_names[0])\n",
    "# images = [cv2.imread('./data/all_ica_img/'+file_name)\n",
    "#                         for file_name in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = sorted(os.listdir('./data/all_ica_img/'))[:]\n",
    "\n",
    "X = np.array([resize(cv2.imread(join('./data/all_ica_img/', file_name))[25:570, 185:720], IMG_SIZE, mode='reflect')\n",
    "              for file_name in file_names[:]]).astype(np.float32)\n",
    "y = np.array([file_name.split('.')[0].split('_')[-1] for file_name in file_names[:]]).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([1700, 5541], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print (np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_majority = X[y==1]\n",
    "X_minority = X[y==0]\n",
    "X_majority_downsampled = resample(X_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=1700,     # to match minority class\n",
    "                                 random_state=123)\n",
    "X = np.concatenate((X_minority,X_majority_downsampled),axis=0)\n",
    "y = np.zeros(1700*2)\n",
    "y[1700:]=1\n",
    "del X_majority\n",
    "del X_minority\n",
    "del X_majority_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = to_categorical(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = './data/train_test/train/'\n",
    "# file_names = sorted(os.listdir(path))\n",
    "# X_train = [resize(cv2.imread(join(path, file_name))[25:570, 185:720], IMG_SIZE, mode='reflect')\n",
    "#               for file_name in file_names]\n",
    "# y_train = [file_name.split('.')[0].split('_')[-1] for file_name in file_names]\n",
    "\n",
    "# X_train = np.array(X_train).astype(np.float32)\n",
    "# y_train = np.array(y_train).astype(int)\n",
    "# y_train = to_categorical(y_train)\n",
    "\n",
    "\n",
    "\n",
    "# path = './data/train_test/test/'\n",
    "# file_names = sorted(os.listdir(path))\n",
    "# X_test = [resize(cv2.imread(join(path, file_name))[25:570, 185:720], IMG_SIZE, mode='reflect')\n",
    "#               for file_name in file_names]\n",
    "# y_test = [file_name.split('.')[0].split('_')[-1] for file_name in file_names]\n",
    "\n",
    "# X_test = np.array(X_test).astype(np.float32)\n",
    "# X_test = np.array(X_test).astype(int)\n",
    "# y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### аугментация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datagen = ImageDataGenerator(rotation_range=10,\n",
    "#                                      width_shift_range=0.1,\n",
    "#                                      height_shift_range=0.1,\n",
    "#                                      horizontal_flip=True)\n",
    "\n",
    "# model.fit_generator(datagen.flow(X_train,y_train,batch_size=128), steps_per_epoch=len(X_train) / 128,\n",
    "#                     epochs=20,\n",
    "#                     verbose=1, \n",
    "#                     validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     2
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception\n",
    "\n",
    "def get_Xception():\n",
    "    basemodel = Xception(include_top=False, input_shape=IMG_SIZE, pooling='avg')\n",
    "    base_out = basemodel.output\n",
    "\n",
    "    base_out = Dense(2048, activation='elu',\n",
    "                           kernel_initializer='glorot_normal',\n",
    "                           kernel_regularizer=l2(1e-3))(base_out)\n",
    "\n",
    "    base_out = Dropout(0.5)(base_out)\n",
    "\n",
    "    base_out = Dense(1024, activation='relu',\n",
    "                           kernel_initializer='glorot_normal',\n",
    "                           kernel_regularizer=l2(1e-3))(base_out)\n",
    "\n",
    "    predictions = Dense(2, activation='softmax',\n",
    "                              kernel_initializer='glorot_normal',\n",
    "                              kernel_regularizer=l2(1e-3))(base_out)\n",
    "\n",
    "    model = Model(inputs=basemodel.input, outputs=predictions)\n",
    "    trainable_border = 107  # 141\n",
    "    for layer in model.layers[:trainable_border]:\n",
    "            layer.trainable = False\n",
    "    for layer in model.layers[trainable_border:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-3), #, decay=1e-2\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2380 samples, validate on 1020 samples\n",
      "Epoch 1/150\n",
      "2380/2380 [==============================] - 42s 18ms/step - loss: 2.5293 - acc: 0.5101 - val_loss: 1.6326 - val_acc: 0.4863\n",
      "Epoch 2/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 1.2322 - acc: 0.4954 - val_loss: 0.9558 - val_acc: 0.4863\n",
      "Epoch 3/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.8542 - acc: 0.5059 - val_loss: 0.7807 - val_acc: 0.4863\n",
      "Epoch 4/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.7502 - acc: 0.4857 - val_loss: 0.7272 - val_acc: 0.4863\n",
      "Epoch 5/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.7169 - acc: 0.4803 - val_loss: 0.7086 - val_acc: 0.4863\n",
      "Epoch 6/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.7044 - acc: 0.4945 - val_loss: 0.7011 - val_acc: 0.4863\n",
      "Epoch 7/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6993 - acc: 0.5059 - val_loss: 0.6979 - val_acc: 0.4863\n",
      "Epoch 8/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6968 - acc: 0.5059 - val_loss: 0.6962 - val_acc: 0.4863\n",
      "Epoch 9/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6956 - acc: 0.5059 - val_loss: 0.6954 - val_acc: 0.4863\n",
      "Epoch 10/150\n",
      "2380/2380 [==============================] - 34s 14ms/step - loss: 0.6949 - acc: 0.5059 - val_loss: 0.6949 - val_acc: 0.4863\n",
      "Epoch 11/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6945 - acc: 0.5059 - val_loss: 0.6946 - val_acc: 0.4863\n",
      "Epoch 12/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6943 - acc: 0.5059 - val_loss: 0.6945 - val_acc: 0.4863\n",
      "Epoch 13/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6940 - acc: 0.5059 - val_loss: 0.6944 - val_acc: 0.4863\n",
      "Epoch 14/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6940 - acc: 0.5059 - val_loss: 0.6944 - val_acc: 0.4863\n",
      "Epoch 15/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6938 - acc: 0.5059 - val_loss: 0.6942 - val_acc: 0.4863\n",
      "Epoch 16/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6937 - acc: 0.5059 - val_loss: 0.6942 - val_acc: 0.4863\n",
      "Epoch 17/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6936 - acc: 0.5059 - val_loss: 0.6940 - val_acc: 0.4863\n",
      "Epoch 18/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6936 - acc: 0.5059 - val_loss: 0.6941 - val_acc: 0.4863\n",
      "Epoch 19/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6935 - acc: 0.5059 - val_loss: 0.6939 - val_acc: 0.4863\n",
      "Epoch 20/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6935 - acc: 0.5059 - val_loss: 0.6938 - val_acc: 0.4863\n",
      "Epoch 21/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6935 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 22/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6934 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 23/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6934 - acc: 0.5059 - val_loss: 0.6938 - val_acc: 0.4863\n",
      "Epoch 24/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6934 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 25/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6933 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 26/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6933 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 27/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6933 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 28/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6933 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 29/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 30/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 31/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 32/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 33/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 34/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6933 - acc: 0.5059 - val_loss: 0.6938 - val_acc: 0.4863\n",
      "Epoch 35/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 36/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 37/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 38/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 39/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 40/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 41/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 42/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 43/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 44/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 45/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 46/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 47/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 48/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 49/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 50/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 51/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 52/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 53/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 54/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6938 - val_acc: 0.4863\n",
      "Epoch 55/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 56/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 57/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 58/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 59/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 61/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 62/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 63/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 64/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 65/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 66/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 67/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 68/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 69/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 70/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 71/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 72/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 73/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 74/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 75/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 76/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 77/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 78/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 79/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 80/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 81/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 82/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 83/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 84/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 85/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 86/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 87/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 88/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 89/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 90/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 91/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 92/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 93/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 94/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 95/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 96/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 97/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 98/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 99/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 100/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 101/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 102/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 103/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 104/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 105/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 106/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 107/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 108/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 109/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 110/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 111/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 112/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 113/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 114/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 115/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 116/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6938 - val_acc: 0.4863\n",
      "Epoch 117/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6938 - val_acc: 0.4863\n",
      "Epoch 118/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6938 - val_acc: 0.4863\n",
      "Epoch 119/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 120/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 121/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 122/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 123/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 124/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 125/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6938 - val_acc: 0.4863\n",
      "Epoch 126/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 127/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 128/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 129/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 130/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 131/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 132/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 133/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4863\n",
      "Epoch 134/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 135/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 136/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 137/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 138/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 139/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 140/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6937 - val_acc: 0.4863\n",
      "Epoch 141/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6938 - val_acc: 0.4863\n",
      "Epoch 142/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 143/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 144/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 145/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 146/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6936 - val_acc: 0.4863\n",
      "Epoch 147/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 148/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6932 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 149/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n",
      "Epoch 150/150\n",
      "2380/2380 [==============================] - 33s 14ms/step - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4863\n"
     ]
    }
   ],
   "source": [
    "model = get_Xception()\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=150,\n",
    "          verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJIAAAG5CAYAAADVvh5NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFAtJkiRJkiRJGoqFJEmSJEmSJA3lbwG89KJ9BYujAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2697cf277f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     1,
     26
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "def get_ResNet():\n",
    "    basemodel = ResNet50(include_top=False, input_shape=IMG_SIZE)\n",
    "    base_out = basemodel.output\n",
    "\n",
    "    base_out = Flatten()(base_out)\n",
    "\n",
    "    base_out = Dense(1024, activation='elu',\n",
    "                           kernel_initializer='glorot_normal',\n",
    "                           kernel_regularizer=l2(1e-3))(base_out)\n",
    "    base_out = Dropout(0.5)(base_out)\n",
    "    predictions = Dense(2, activation='softmax',\n",
    "                              kernel_initializer='glorot_normal',\n",
    "                              kernel_regularizer=l2(1e-3))(base_out)\n",
    "    model = Model(inputs=basemodel.input, outputs=predictions)\n",
    "\n",
    "    for layer in model.layers[:141]:\n",
    "                layer.trainable = False\n",
    "    for layer in model.layers[141:]:\n",
    "                layer.trainable = True\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-4, decay=1e-2),\n",
    "                        loss='categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#\n",
    "    # model = ResNet50(include_top=False,input_shape = IMG_SIZE)\n",
    "    # for layer in model.layers[:141]:\n",
    "    #             layer.trainable = False\n",
    "    # for layer in model.layers[141:]:\n",
    "    #             layer.trainable = True       \n",
    "    # last = model.layers[-1].output\n",
    "    # x = Dense(2, activation=\"softmax\")(last)\n",
    "    # finetuned_model = Model(model.input, x)\n",
    "    # finetuned_model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # history = finetuned_model.fit(X_train, y_train, batch_size=128, epochs=10,\n",
    "    #           verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Train on 2380 samples, validate on 1020 samples\n",
      "Epoch 1/150\n",
      "2380/2380 [==============================] - 37s 15ms/step - loss: 1.7360 - acc: 0.6517 - val_loss: 1.9354 - val_acc: 0.4863\n",
      "Epoch 2/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 1.4397 - acc: 0.8181 - val_loss: 1.8544 - val_acc: 0.4863\n",
      "Epoch 3/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 1.2483 - acc: 0.9151 - val_loss: 1.9079 - val_acc: 0.4863\n",
      "Epoch 4/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 1.0948 - acc: 0.9819 - val_loss: 2.2047 - val_acc: 0.4863\n",
      "Epoch 5/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 1.0339 - acc: 0.9941 - val_loss: 2.2019 - val_acc: 0.4863\n",
      "Epoch 6/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 1.0040 - acc: 0.9966 - val_loss: 2.1433 - val_acc: 0.4863\n",
      "Epoch 7/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.9856 - acc: 0.9987 - val_loss: 2.1849 - val_acc: 0.4863\n",
      "Epoch 8/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.9721 - acc: 0.9992 - val_loss: 2.1218 - val_acc: 0.4863\n",
      "Epoch 9/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.9683 - acc: 0.9966 - val_loss: 2.0979 - val_acc: 0.4863\n",
      "Epoch 10/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.9606 - acc: 0.9975 - val_loss: 2.1567 - val_acc: 0.4863\n",
      "Epoch 11/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.9509 - acc: 0.9996 - val_loss: 1.9377 - val_acc: 0.4863\n",
      "Epoch 12/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.9410 - acc: 0.9987 - val_loss: 1.9493 - val_acc: 0.4863\n",
      "Epoch 13/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.9332 - acc: 0.9996 - val_loss: 1.9038 - val_acc: 0.4863\n",
      "Epoch 14/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.9249 - acc: 0.9996 - val_loss: 1.9713 - val_acc: 0.4863\n",
      "Epoch 15/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.9181 - acc: 0.9996 - val_loss: 1.8918 - val_acc: 0.4863\n",
      "Epoch 16/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.9125 - acc: 1.0000 - val_loss: 1.9785 - val_acc: 0.4863\n",
      "Epoch 17/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.9057 - acc: 1.0000 - val_loss: 1.9544 - val_acc: 0.4863\n",
      "Epoch 18/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8996 - acc: 1.0000 - val_loss: 1.9632 - val_acc: 0.4863\n",
      "Epoch 19/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8943 - acc: 1.0000 - val_loss: 1.9453 - val_acc: 0.4863\n",
      "Epoch 20/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8889 - acc: 1.0000 - val_loss: 1.9127 - val_acc: 0.4863\n",
      "Epoch 21/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8839 - acc: 1.0000 - val_loss: 1.9036 - val_acc: 0.4863\n",
      "Epoch 22/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8788 - acc: 1.0000 - val_loss: 1.8820 - val_acc: 0.4863\n",
      "Epoch 23/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8741 - acc: 1.0000 - val_loss: 1.8733 - val_acc: 0.4863\n",
      "Epoch 24/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8697 - acc: 1.0000 - val_loss: 1.8736 - val_acc: 0.4863\n",
      "Epoch 25/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8654 - acc: 1.0000 - val_loss: 1.8338 - val_acc: 0.4863\n",
      "Epoch 26/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8617 - acc: 1.0000 - val_loss: 1.8969 - val_acc: 0.4863\n",
      "Epoch 27/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8571 - acc: 1.0000 - val_loss: 1.9991 - val_acc: 0.4863\n",
      "Epoch 28/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8524 - acc: 1.0000 - val_loss: 2.0051 - val_acc: 0.4863\n",
      "Epoch 29/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8481 - acc: 1.0000 - val_loss: 1.9194 - val_acc: 0.4863\n",
      "Epoch 30/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8441 - acc: 1.0000 - val_loss: 1.9536 - val_acc: 0.4863\n",
      "Epoch 31/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8402 - acc: 1.0000 - val_loss: 1.9350 - val_acc: 0.4863\n",
      "Epoch 32/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8367 - acc: 1.0000 - val_loss: 1.8893 - val_acc: 0.4863\n",
      "Epoch 33/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8325 - acc: 1.0000 - val_loss: 1.9727 - val_acc: 0.4863\n",
      "Epoch 34/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8283 - acc: 1.0000 - val_loss: 1.9715 - val_acc: 0.4863\n",
      "Epoch 35/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8256 - acc: 0.9996 - val_loss: 1.9958 - val_acc: 0.4863\n",
      "Epoch 36/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8225 - acc: 0.9996 - val_loss: 1.9845 - val_acc: 0.4863\n",
      "Epoch 37/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8189 - acc: 0.9996 - val_loss: 1.9929 - val_acc: 0.4863\n",
      "Epoch 38/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8141 - acc: 1.0000 - val_loss: 1.9958 - val_acc: 0.4863\n",
      "Epoch 39/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8109 - acc: 0.9996 - val_loss: 1.9599 - val_acc: 0.4863\n",
      "Epoch 40/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8073 - acc: 1.0000 - val_loss: 1.8998 - val_acc: 0.4863\n",
      "Epoch 41/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8042 - acc: 1.0000 - val_loss: 1.9092 - val_acc: 0.4863\n",
      "Epoch 42/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.8006 - acc: 1.0000 - val_loss: 1.9913 - val_acc: 0.4863\n",
      "Epoch 43/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7988 - acc: 0.9996 - val_loss: 1.9359 - val_acc: 0.4863\n",
      "Epoch 44/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7941 - acc: 1.0000 - val_loss: 1.8081 - val_acc: 0.4863\n",
      "Epoch 45/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7909 - acc: 1.0000 - val_loss: 1.8186 - val_acc: 0.4863\n",
      "Epoch 46/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7875 - acc: 1.0000 - val_loss: 1.8329 - val_acc: 0.4863\n",
      "Epoch 47/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7854 - acc: 0.9996 - val_loss: 1.8356 - val_acc: 0.4863\n",
      "Epoch 48/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7821 - acc: 1.0000 - val_loss: 1.9338 - val_acc: 0.4863\n",
      "Epoch 49/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7787 - acc: 1.0000 - val_loss: 1.9078 - val_acc: 0.4863\n",
      "Epoch 50/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7756 - acc: 1.0000 - val_loss: 1.8948 - val_acc: 0.4863\n",
      "Epoch 51/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7725 - acc: 1.0000 - val_loss: 1.8619 - val_acc: 0.4863\n",
      "Epoch 52/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7697 - acc: 1.0000 - val_loss: 1.8111 - val_acc: 0.4863\n",
      "Epoch 53/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7668 - acc: 1.0000 - val_loss: 1.7972 - val_acc: 0.4863\n",
      "Epoch 54/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7639 - acc: 1.0000 - val_loss: 1.7836 - val_acc: 0.4863\n",
      "Epoch 55/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7611 - acc: 1.0000 - val_loss: 1.8093 - val_acc: 0.4863\n",
      "Epoch 56/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7584 - acc: 1.0000 - val_loss: 1.7999 - val_acc: 0.4863\n",
      "Epoch 57/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7554 - acc: 1.0000 - val_loss: 1.8380 - val_acc: 0.4863\n",
      "Epoch 58/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7527 - acc: 1.0000 - val_loss: 1.8263 - val_acc: 0.4863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7498 - acc: 1.0000 - val_loss: 1.8332 - val_acc: 0.4863\n",
      "Epoch 60/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7475 - acc: 0.9996 - val_loss: 1.8160 - val_acc: 0.4863\n",
      "Epoch 61/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7456 - acc: 0.9996 - val_loss: 1.8060 - val_acc: 0.4863\n",
      "Epoch 62/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7424 - acc: 1.0000 - val_loss: 1.7684 - val_acc: 0.4863\n",
      "Epoch 63/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7395 - acc: 1.0000 - val_loss: 1.8054 - val_acc: 0.4863\n",
      "Epoch 64/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7367 - acc: 1.0000 - val_loss: 1.7927 - val_acc: 0.4863\n",
      "Epoch 65/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7343 - acc: 1.0000 - val_loss: 1.8007 - val_acc: 0.4863\n",
      "Epoch 66/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7316 - acc: 1.0000 - val_loss: 1.7865 - val_acc: 0.4863\n",
      "Epoch 67/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7291 - acc: 1.0000 - val_loss: 1.7765 - val_acc: 0.4863\n",
      "Epoch 68/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7266 - acc: 1.0000 - val_loss: 1.7638 - val_acc: 0.4863\n",
      "Epoch 69/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7240 - acc: 1.0000 - val_loss: 1.7556 - val_acc: 0.4863\n",
      "Epoch 70/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7215 - acc: 1.0000 - val_loss: 1.7474 - val_acc: 0.4863\n",
      "Epoch 71/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7190 - acc: 1.0000 - val_loss: 1.7562 - val_acc: 0.4863\n",
      "Epoch 72/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7182 - acc: 0.9996 - val_loss: 1.7395 - val_acc: 0.4863\n",
      "Epoch 73/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7145 - acc: 1.0000 - val_loss: 1.7759 - val_acc: 0.4863\n",
      "Epoch 74/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7120 - acc: 1.0000 - val_loss: 1.7781 - val_acc: 0.4863\n",
      "Epoch 75/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7096 - acc: 1.0000 - val_loss: 1.7754 - val_acc: 0.4863\n",
      "Epoch 76/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7073 - acc: 1.0000 - val_loss: 1.7878 - val_acc: 0.4863\n",
      "Epoch 77/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7048 - acc: 1.0000 - val_loss: 1.7846 - val_acc: 0.4863\n",
      "Epoch 78/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7026 - acc: 1.0000 - val_loss: 1.7710 - val_acc: 0.4863\n",
      "Epoch 79/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.7001 - acc: 1.0000 - val_loss: 1.7591 - val_acc: 0.4863\n",
      "Epoch 80/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6977 - acc: 1.0000 - val_loss: 1.7548 - val_acc: 0.4863\n",
      "Epoch 81/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6955 - acc: 1.0000 - val_loss: 1.7505 - val_acc: 0.4863\n",
      "Epoch 82/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6930 - acc: 1.0000 - val_loss: 1.7556 - val_acc: 0.4863\n",
      "Epoch 83/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6907 - acc: 1.0000 - val_loss: 1.7530 - val_acc: 0.4863\n",
      "Epoch 84/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6888 - acc: 1.0000 - val_loss: 1.7447 - val_acc: 0.4863\n",
      "Epoch 85/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6865 - acc: 1.0000 - val_loss: 1.7329 - val_acc: 0.4863\n",
      "Epoch 86/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6841 - acc: 1.0000 - val_loss: 1.7373 - val_acc: 0.4863\n",
      "Epoch 87/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6817 - acc: 1.0000 - val_loss: 1.7216 - val_acc: 0.4863\n",
      "Epoch 88/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6795 - acc: 1.0000 - val_loss: 1.7229 - val_acc: 0.4863\n",
      "Epoch 89/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6772 - acc: 1.0000 - val_loss: 1.7132 - val_acc: 0.4863\n",
      "Epoch 90/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6750 - acc: 1.0000 - val_loss: 1.7072 - val_acc: 0.4863\n",
      "Epoch 91/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6727 - acc: 1.0000 - val_loss: 1.7058 - val_acc: 0.4863\n",
      "Epoch 92/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6704 - acc: 1.0000 - val_loss: 1.6915 - val_acc: 0.4863\n",
      "Epoch 93/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6682 - acc: 1.0000 - val_loss: 1.6872 - val_acc: 0.4863\n",
      "Epoch 94/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6660 - acc: 1.0000 - val_loss: 1.6904 - val_acc: 0.4863\n",
      "Epoch 95/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6638 - acc: 1.0000 - val_loss: 1.6946 - val_acc: 0.4863\n",
      "Epoch 96/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6616 - acc: 1.0000 - val_loss: 1.7429 - val_acc: 0.4863\n",
      "Epoch 97/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6594 - acc: 1.0000 - val_loss: 1.7815 - val_acc: 0.4863\n",
      "Epoch 98/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6573 - acc: 1.0000 - val_loss: 1.7649 - val_acc: 0.4863\n",
      "Epoch 99/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6554 - acc: 1.0000 - val_loss: 1.7933 - val_acc: 0.4863\n",
      "Epoch 100/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6531 - acc: 1.0000 - val_loss: 1.8485 - val_acc: 0.4863\n",
      "Epoch 101/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6509 - acc: 1.0000 - val_loss: 1.8027 - val_acc: 0.4863\n",
      "Epoch 102/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6487 - acc: 1.0000 - val_loss: 1.7814 - val_acc: 0.4863\n",
      "Epoch 103/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6465 - acc: 1.0000 - val_loss: 1.7666 - val_acc: 0.4863\n",
      "Epoch 104/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6443 - acc: 1.0000 - val_loss: 1.7570 - val_acc: 0.4863\n",
      "Epoch 105/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6422 - acc: 1.0000 - val_loss: 1.7263 - val_acc: 0.4863\n",
      "Epoch 106/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6402 - acc: 1.0000 - val_loss: 1.7220 - val_acc: 0.4863\n",
      "Epoch 107/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6380 - acc: 1.0000 - val_loss: 1.6932 - val_acc: 0.4863\n",
      "Epoch 108/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6358 - acc: 1.0000 - val_loss: 1.6873 - val_acc: 0.4863\n",
      "Epoch 109/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6340 - acc: 1.0000 - val_loss: 1.6866 - val_acc: 0.4863\n",
      "Epoch 110/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6319 - acc: 1.0000 - val_loss: 1.6594 - val_acc: 0.4863\n",
      "Epoch 111/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6298 - acc: 1.0000 - val_loss: 1.7104 - val_acc: 0.4863\n",
      "Epoch 112/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6279 - acc: 1.0000 - val_loss: 1.7192 - val_acc: 0.4863\n",
      "Epoch 113/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6262 - acc: 1.0000 - val_loss: 1.7465 - val_acc: 0.4863\n",
      "Epoch 114/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6238 - acc: 1.0000 - val_loss: 1.6659 - val_acc: 0.4863\n",
      "Epoch 115/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6219 - acc: 1.0000 - val_loss: 1.6724 - val_acc: 0.4863\n",
      "Epoch 116/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6200 - acc: 1.0000 - val_loss: 1.6830 - val_acc: 0.4863\n",
      "Epoch 117/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6178 - acc: 1.0000 - val_loss: 1.7066 - val_acc: 0.4863\n",
      "Epoch 118/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6159 - acc: 1.0000 - val_loss: 1.7034 - val_acc: 0.4863\n",
      "Epoch 119/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6140 - acc: 1.0000 - val_loss: 1.7065 - val_acc: 0.4863\n",
      "Epoch 120/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6120 - acc: 1.0000 - val_loss: 1.7222 - val_acc: 0.4863\n",
      "Epoch 121/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6101 - acc: 1.0000 - val_loss: 1.7141 - val_acc: 0.4863\n",
      "Epoch 122/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6081 - acc: 1.0000 - val_loss: 1.7004 - val_acc: 0.4863\n",
      "Epoch 123/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6067 - acc: 0.9996 - val_loss: 1.7207 - val_acc: 0.4863\n",
      "Epoch 124/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6061 - acc: 0.9996 - val_loss: 1.7496 - val_acc: 0.4863\n",
      "Epoch 125/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6027 - acc: 1.0000 - val_loss: 1.7499 - val_acc: 0.4863\n",
      "Epoch 126/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.6008 - acc: 1.0000 - val_loss: 1.7546 - val_acc: 0.4863\n",
      "Epoch 127/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5990 - acc: 1.0000 - val_loss: 1.7525 - val_acc: 0.4863\n",
      "Epoch 128/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5972 - acc: 1.0000 - val_loss: 1.7533 - val_acc: 0.4863\n",
      "Epoch 129/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5953 - acc: 1.0000 - val_loss: 1.7721 - val_acc: 0.4863\n",
      "Epoch 130/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5936 - acc: 1.0000 - val_loss: 1.7840 - val_acc: 0.4863\n",
      "Epoch 131/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5917 - acc: 1.0000 - val_loss: 1.7682 - val_acc: 0.4863\n",
      "Epoch 132/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5898 - acc: 1.0000 - val_loss: 1.7551 - val_acc: 0.4863\n",
      "Epoch 133/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5880 - acc: 1.0000 - val_loss: 1.7442 - val_acc: 0.4863\n",
      "Epoch 134/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5862 - acc: 1.0000 - val_loss: 1.7349 - val_acc: 0.4863\n",
      "Epoch 135/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5845 - acc: 1.0000 - val_loss: 1.7237 - val_acc: 0.4863\n",
      "Epoch 136/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5826 - acc: 1.0000 - val_loss: 1.7128 - val_acc: 0.4863\n",
      "Epoch 137/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5809 - acc: 1.0000 - val_loss: 1.7033 - val_acc: 0.4863\n",
      "Epoch 138/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5791 - acc: 1.0000 - val_loss: 1.7369 - val_acc: 0.4863\n",
      "Epoch 139/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5772 - acc: 1.0000 - val_loss: 1.7318 - val_acc: 0.4863\n",
      "Epoch 140/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5755 - acc: 1.0000 - val_loss: 1.7051 - val_acc: 0.4863\n",
      "Epoch 141/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5737 - acc: 1.0000 - val_loss: 1.6294 - val_acc: 0.4863\n",
      "Epoch 142/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5722 - acc: 1.0000 - val_loss: 1.6185 - val_acc: 0.4863\n",
      "Epoch 143/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5703 - acc: 1.0000 - val_loss: 1.6395 - val_acc: 0.4863\n",
      "Epoch 144/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5685 - acc: 1.0000 - val_loss: 1.6216 - val_acc: 0.4863\n",
      "Epoch 145/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5668 - acc: 1.0000 - val_loss: 1.6372 - val_acc: 0.4863\n",
      "Epoch 146/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5650 - acc: 1.0000 - val_loss: 1.6961 - val_acc: 0.4863\n",
      "Epoch 147/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5633 - acc: 1.0000 - val_loss: 1.6749 - val_acc: 0.4863\n",
      "Epoch 148/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5616 - acc: 1.0000 - val_loss: 1.6525 - val_acc: 0.4863\n",
      "Epoch 149/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5600 - acc: 1.0000 - val_loss: 1.6651 - val_acc: 0.4863\n",
      "Epoch 150/150\n",
      "2380/2380 [==============================] - 31s 13ms/step - loss: 0.5582 - acc: 1.0000 - val_loss: 1.7152 - val_acc: 0.4863\n"
     ]
    }
   ],
   "source": [
    "model = get_ResNet()\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=150,\n",
    "          verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJkAAAG5CAYAAAAtXuW6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2/8Pmt7j6kQ2H/gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26c6a6a0588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "def get_VGG16():\n",
    "    model = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = (IMG_SIZE))\n",
    "    for layer in model.layers[:5]:\n",
    "        layer.trainable = False\n",
    "    x = model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    predictions = Dense(2, activation=\"softmax\")(x)\n",
    "    model_final = Model(inputs = model.input, outputs = predictions)\n",
    "    model_final.compile(loss = \"categorical_crossentropy\", optimizer = SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "    return model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2380 samples, validate on 1020 samples\n",
      "Epoch 1/150\n",
      "2380/2380 [==============================] - 73s 31ms/step - loss: 0.7092 - acc: 0.5639 - val_loss: 0.6237 - val_acc: 0.6471\n",
      "Epoch 2/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.6456 - acc: 0.6256 - val_loss: 0.5991 - val_acc: 0.6490\n",
      "Epoch 3/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.6275 - acc: 0.6534 - val_loss: 0.5946 - val_acc: 0.6706\n",
      "Epoch 4/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.6055 - acc: 0.6807 - val_loss: 0.5905 - val_acc: 0.6716\n",
      "Epoch 5/150\n",
      "2380/2380 [==============================] - 59s 25ms/step - loss: 0.6024 - acc: 0.6756 - val_loss: 0.5873 - val_acc: 0.6824\n",
      "Epoch 6/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.5899 - acc: 0.6920 - val_loss: 0.6057 - val_acc: 0.6804\n",
      "Epoch 7/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.5872 - acc: 0.7021 - val_loss: 0.5752 - val_acc: 0.6814\n",
      "Epoch 8/150\n",
      "2380/2380 [==============================] - 61s 25ms/step - loss: 0.5605 - acc: 0.7181 - val_loss: 0.5763 - val_acc: 0.6863\n",
      "Epoch 9/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.5619 - acc: 0.7088 - val_loss: 0.5727 - val_acc: 0.6912\n",
      "Epoch 10/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.5464 - acc: 0.7084 - val_loss: 0.5824 - val_acc: 0.6902\n",
      "Epoch 11/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.5398 - acc: 0.7282 - val_loss: 0.5771 - val_acc: 0.6961\n",
      "Epoch 12/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.5321 - acc: 0.7311 - val_loss: 0.5667 - val_acc: 0.6843\n",
      "Epoch 13/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.5241 - acc: 0.7441 - val_loss: 0.5707 - val_acc: 0.6941\n",
      "Epoch 14/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.5149 - acc: 0.7475 - val_loss: 0.5669 - val_acc: 0.6843\n",
      "Epoch 15/150\n",
      "2380/2380 [==============================] - 59s 25ms/step - loss: 0.5036 - acc: 0.7450 - val_loss: 0.5653 - val_acc: 0.6951\n",
      "Epoch 16/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.4979 - acc: 0.7542 - val_loss: 0.5631 - val_acc: 0.6873\n",
      "Epoch 17/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.4792 - acc: 0.7748 - val_loss: 0.5728 - val_acc: 0.6961\n",
      "Epoch 18/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.4768 - acc: 0.7681 - val_loss: 0.5799 - val_acc: 0.6951\n",
      "Epoch 19/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.4649 - acc: 0.7664 - val_loss: 0.5655 - val_acc: 0.6971\n",
      "Epoch 20/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.4513 - acc: 0.7903 - val_loss: 0.5626 - val_acc: 0.6990\n",
      "Epoch 21/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.4456 - acc: 0.7878 - val_loss: 0.5727 - val_acc: 0.7078\n",
      "Epoch 22/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.4269 - acc: 0.7983 - val_loss: 0.5657 - val_acc: 0.7049\n",
      "Epoch 23/150\n",
      "2380/2380 [==============================] - 61s 25ms/step - loss: 0.4106 - acc: 0.8134 - val_loss: 0.5821 - val_acc: 0.7069\n",
      "Epoch 24/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.4045 - acc: 0.8235 - val_loss: 0.5861 - val_acc: 0.7000\n",
      "Epoch 25/150\n",
      "2380/2380 [==============================] - 59s 25ms/step - loss: 0.3989 - acc: 0.8235 - val_loss: 0.5936 - val_acc: 0.7010\n",
      "Epoch 26/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.3786 - acc: 0.8357 - val_loss: 0.6242 - val_acc: 0.6951\n",
      "Epoch 27/150\n",
      "2380/2380 [==============================] - 62s 26ms/step - loss: 0.3847 - acc: 0.8256 - val_loss: 0.5940 - val_acc: 0.7029\n",
      "Epoch 28/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.3662 - acc: 0.8382 - val_loss: 0.5919 - val_acc: 0.7029\n",
      "Epoch 29/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.3412 - acc: 0.8483 - val_loss: 0.6267 - val_acc: 0.6980\n",
      "Epoch 30/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.3229 - acc: 0.8559 - val_loss: 0.6126 - val_acc: 0.7069\n",
      "Epoch 31/150\n",
      "2380/2380 [==============================] - 62s 26ms/step - loss: 0.3088 - acc: 0.8660 - val_loss: 0.6193 - val_acc: 0.7039\n",
      "Epoch 32/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.2893 - acc: 0.8794 - val_loss: 0.6374 - val_acc: 0.6971\n",
      "Epoch 33/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.2809 - acc: 0.8794 - val_loss: 0.6718 - val_acc: 0.7029\n",
      "Epoch 34/150\n",
      "2380/2380 [==============================] - 59s 25ms/step - loss: 0.2663 - acc: 0.8916 - val_loss: 0.6468 - val_acc: 0.7118\n",
      "Epoch 35/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.2500 - acc: 0.9029 - val_loss: 0.6544 - val_acc: 0.6980\n",
      "Epoch 36/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.2245 - acc: 0.9143 - val_loss: 0.6894 - val_acc: 0.7088\n",
      "Epoch 37/150\n",
      "2380/2380 [==============================] - 62s 26ms/step - loss: 0.2190 - acc: 0.9206 - val_loss: 0.8310 - val_acc: 0.6863\n",
      "Epoch 38/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.2172 - acc: 0.9160 - val_loss: 0.7150 - val_acc: 0.7020\n",
      "Epoch 39/150\n",
      "2380/2380 [==============================] - 61s 25ms/step - loss: 0.1888 - acc: 0.9315 - val_loss: 0.7415 - val_acc: 0.6971\n",
      "Epoch 40/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.1727 - acc: 0.9378 - val_loss: 0.8492 - val_acc: 0.6863\n",
      "Epoch 41/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.1716 - acc: 0.9336 - val_loss: 0.7841 - val_acc: 0.7010\n",
      "Epoch 42/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.1764 - acc: 0.9319 - val_loss: 0.7393 - val_acc: 0.7020\n",
      "Epoch 43/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.1418 - acc: 0.9521 - val_loss: 0.9085 - val_acc: 0.6931\n",
      "Epoch 44/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.1308 - acc: 0.9559 - val_loss: 0.8739 - val_acc: 0.7010\n",
      "Epoch 45/150\n",
      "2380/2380 [==============================] - 60s 25ms/step - loss: 0.1248 - acc: 0.9605 - val_loss: 0.8152 - val_acc: 0.7020\n",
      "Epoch 46/150\n",
      "2380/2380 [==============================] - 61s 26ms/step - loss: 0.1233 - acc: 0.9546 - val_loss: 0.8623 - val_acc: 0.7059\n",
      "Epoch 47/150\n",
      "1536/2380 [==================>...........] - ETA: 18s - loss: 0.1040 - acc: 0.9701"
     ]
    }
   ],
   "source": [
    "model = get_VGG16()\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=150,\n",
    "          verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     9
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Lambda,GlobalAveragePooling2D\n",
    "def ConvBlock(model, layers, filters):\n",
    "    '''Create [layers] layers consisting of zero padding, a convolution with [filters] 3x3 filters and batch normalization. Perform max pooling after the last layer.'''\n",
    "    for i in range(layers):\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Conv2D(filters, (3, 3), activation='relu'))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "def create_model():\n",
    "    '''Create the FCN and return a keras model.'''\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input image: 75x75x3\n",
    "    model.add(Lambda(lambda x: x, input_shape=IMG_SIZE))\n",
    "    ConvBlock(model, 1, 32)\n",
    "    # 37x37x32\n",
    "    ConvBlock(model, 1, 64)\n",
    "    # 18x18x64\n",
    "    ConvBlock(model, 1, 128)\n",
    "    # 9x9x128\n",
    "    ConvBlock(model, 1, 128)\n",
    "    # 4x4x128\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(2, (3, 3), activation='relu'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    # 4x4x2\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=150,\n",
    "          verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    #Build keras model\n",
    "    \n",
    "    model=Sequential()\n",
    "    \n",
    "    # CNN 1\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=IMG_SIZE))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # CNN 2\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # CNN 3\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #CNN 4\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # You must flatten the data for the dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #Dense 1\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Dense 2\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    optimizer = Adam(lr=0.001, decay=0.0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = getModel()\n",
    "# batch_size = 32\n",
    "# earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "# mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "history = model.fit(X_train, y_train[:,1], batch_size=64, epochs=150,\n",
    "          verbose=1, validation_data=(X_test, y_test[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_notebook( relu_type='relu'):\n",
    "    # angle variable defines if we should use angle parameter or ignore it\n",
    "    input_1 = Input(shape=IMG_SIZE)\n",
    "\n",
    "    fcnn = Conv2D(32, kernel_size=(3, 3), activation=relu_type)(\n",
    "        BatchNormalization()(input_1))\n",
    "    fcnn = MaxPooling2D((3, 3))(fcnn)\n",
    "    fcnn = Dropout(0.2)(fcnn)\n",
    "    fcnn = Conv2D(64, kernel_size=(3, 3), activation=relu_type)(fcnn)\n",
    "    fcnn = MaxPooling2D((2, 2), strides=(2, 2))(fcnn)\n",
    "    fcnn = Dropout(0.2)(fcnn)\n",
    "    fcnn = Conv2D(128, kernel_size=(3, 3), activation=relu_type)(fcnn)\n",
    "    fcnn = MaxPooling2D((2, 2), strides=(2, 2))(fcnn)\n",
    "    fcnn = Dropout(0.2)(fcnn)\n",
    "    fcnn = Conv2D(128, kernel_size=(3, 3), activation=relu_type)(fcnn)\n",
    "    fcnn = MaxPooling2D((2, 2), strides=(2, 2))(fcnn)\n",
    "    fcnn = Dropout(0.2)(fcnn)\n",
    "    fcnn = BatchNormalization()(fcnn)\n",
    "    fcnn = Flatten()(fcnn)\n",
    "    local_input = input_1\n",
    "    partial_model = Model(input_1, fcnn)\n",
    "    dense = Dropout(0.2)(fcnn)\n",
    "    dense = Dense(256, activation=relu_type)(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    dense = Dense(128, activation=relu_type)(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    dense = Dense(64, activation=relu_type)(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    # For some reason i've decided not to normalize angle data\n",
    "    output = Dense(1, activation=\"sigmoid\")(dense)\n",
    "    model = Model(local_input, output)\n",
    "    optimizer = Adam(lr=0.001, decay=0.001)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model #, partial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_notebook()\n",
    "history = model.fit(X_train, y_train[:,1], batch_size=64, epochs=150,\n",
    "          verbose=1, validation_data=(X_test, y_test[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LeNet_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(6, (5, 5), input_shape = IMG_SIZE))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    model.add(Conv2D(16, (3, 3), padding='valid'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    model.add(Conv2D(120, (1, 1), padding='valid'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet_model()\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=200,\n",
    "          verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, GlobalMaxPooling2D, concatenate, Concatenate\n",
    "from keras.applications import VGG16\n",
    "def getVggAngleModel():\n",
    "\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, \n",
    "                 input_shape=IMG_SIZE, classes=1)\n",
    "    x = base_model.get_layer('block5_pool').output\n",
    "    \n",
    "\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "   \n",
    "    x = Dense(512, activation='relu', name='fc2')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu', name='fc3')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=[base_model.input], outputs=predictions)\n",
    "    \n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = getVggAngleModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train[:,1], batch_size=32, epochs=60,\n",
    "          verbose=1, validation_data=(X_test, y_test[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LeNetPlus():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(Conv2D(96, (11, 11), input_shape = IMG_SIZE, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Conv2D(256, (5, 5), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "    # Layer 6\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Layer 8\n",
    "    model.add(Dense(2, kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(),metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNetPlus()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=12, epochs=150,\n",
    "          verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5068 samples, validate on 2173 samples\n",
      "Epoch 1/150\n",
      "5068/5068 [==============================] - 6s 1ms/step - loss: 0.5904 - acc: 0.7516 - val_loss: 0.5401 - val_acc: 0.7644\n",
      "Epoch 2/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.5457 - acc: 0.7656 - val_loss: 0.5361 - val_acc: 0.7644\n",
      "Epoch 3/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.5338 - acc: 0.7654 - val_loss: 0.5145 - val_acc: 0.7644\n",
      "Epoch 4/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.5327 - acc: 0.7650 - val_loss: 0.5125 - val_acc: 0.7644\n",
      "Epoch 5/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.5238 - acc: 0.7658 - val_loss: 0.5225 - val_acc: 0.7635\n",
      "Epoch 6/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.5231 - acc: 0.7636 - val_loss: 0.5067 - val_acc: 0.7644\n",
      "Epoch 7/150\n",
      "5068/5068 [==============================] - 6s 1ms/step - loss: 0.5179 - acc: 0.7648 - val_loss: 0.4967 - val_acc: 0.7644\n",
      "Epoch 8/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.5130 - acc: 0.7650 - val_loss: 0.4972 - val_acc: 0.7644\n",
      "Epoch 9/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.5093 - acc: 0.7676 - val_loss: 0.4963 - val_acc: 0.7644\n",
      "Epoch 10/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.5092 - acc: 0.7693 - val_loss: 0.4933 - val_acc: 0.7644\n",
      "Epoch 11/150\n",
      "5068/5068 [==============================] - 6s 1ms/step - loss: 0.5055 - acc: 0.7656 - val_loss: 0.4931 - val_acc: 0.7658\n",
      "Epoch 12/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.5052 - acc: 0.7670 - val_loss: 0.4929 - val_acc: 0.7653\n",
      "Epoch 13/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4993 - acc: 0.7691 - val_loss: 0.4851 - val_acc: 0.7768\n",
      "Epoch 14/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.5004 - acc: 0.7727 - val_loss: 0.4855 - val_acc: 0.7763\n",
      "Epoch 15/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4969 - acc: 0.7735 - val_loss: 0.4933 - val_acc: 0.7694\n",
      "Epoch 16/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4944 - acc: 0.7782 - val_loss: 0.4849 - val_acc: 0.7915\n",
      "Epoch 17/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4960 - acc: 0.7782 - val_loss: 0.4802 - val_acc: 0.7906\n",
      "Epoch 18/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4922 - acc: 0.7808 - val_loss: 0.4818 - val_acc: 0.7897\n",
      "Epoch 19/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4916 - acc: 0.7768 - val_loss: 0.4765 - val_acc: 0.7823\n",
      "Epoch 20/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4873 - acc: 0.7835 - val_loss: 0.4707 - val_acc: 0.7957\n",
      "Epoch 21/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4903 - acc: 0.7798 - val_loss: 0.4712 - val_acc: 0.7938\n",
      "Epoch 22/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4833 - acc: 0.7867 - val_loss: 0.4709 - val_acc: 0.7952\n",
      "Epoch 23/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4814 - acc: 0.7881 - val_loss: 0.4691 - val_acc: 0.7929\n",
      "Epoch 24/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4789 - acc: 0.7940 - val_loss: 0.4609 - val_acc: 0.8007\n",
      "Epoch 25/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4756 - acc: 0.7948 - val_loss: 0.4607 - val_acc: 0.8021\n",
      "Epoch 26/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4753 - acc: 0.7958 - val_loss: 0.4571 - val_acc: 0.7994\n",
      "Epoch 27/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4701 - acc: 0.7962 - val_loss: 0.4609 - val_acc: 0.7975\n",
      "Epoch 28/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4663 - acc: 0.8005 - val_loss: 0.4574 - val_acc: 0.8017\n",
      "Epoch 29/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4699 - acc: 0.7940 - val_loss: 0.4637 - val_acc: 0.8003\n",
      "Epoch 30/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4613 - acc: 0.8072 - val_loss: 0.4674 - val_acc: 0.7975\n",
      "Epoch 31/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4596 - acc: 0.8005 - val_loss: 0.4577 - val_acc: 0.8035\n",
      "Epoch 32/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4600 - acc: 0.8031 - val_loss: 0.4520 - val_acc: 0.8021\n",
      "Epoch 33/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4579 - acc: 0.8049 - val_loss: 0.4563 - val_acc: 0.8090\n",
      "Epoch 34/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4541 - acc: 0.8031 - val_loss: 0.4565 - val_acc: 0.8086\n",
      "Epoch 35/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4540 - acc: 0.8047 - val_loss: 0.4537 - val_acc: 0.8109\n",
      "Epoch 36/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4533 - acc: 0.8080 - val_loss: 0.4519 - val_acc: 0.8109\n",
      "Epoch 37/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4550 - acc: 0.8054 - val_loss: 0.4491 - val_acc: 0.8095\n",
      "Epoch 38/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4511 - acc: 0.8072 - val_loss: 0.4521 - val_acc: 0.8086\n",
      "Epoch 39/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4450 - acc: 0.8143 - val_loss: 0.4602 - val_acc: 0.8076\n",
      "Epoch 40/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4463 - acc: 0.8114 - val_loss: 0.4544 - val_acc: 0.8072\n",
      "Epoch 41/150\n",
      "5068/5068 [==============================] - 7s 1ms/step - loss: 0.4464 - acc: 0.8076 - val_loss: 0.4610 - val_acc: 0.7943\n",
      "Epoch 42/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4438 - acc: 0.8118 - val_loss: 0.4624 - val_acc: 0.8067\n",
      "Epoch 43/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4359 - acc: 0.8118 - val_loss: 0.4534 - val_acc: 0.8076\n",
      "Epoch 44/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4402 - acc: 0.8112 - val_loss: 0.4576 - val_acc: 0.8081\n",
      "Epoch 45/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4361 - acc: 0.8129 - val_loss: 0.4535 - val_acc: 0.8090\n",
      "Epoch 46/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4310 - acc: 0.8163 - val_loss: 0.4629 - val_acc: 0.8072\n",
      "Epoch 47/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4347 - acc: 0.8139 - val_loss: 0.4543 - val_acc: 0.8132\n",
      "Epoch 48/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4343 - acc: 0.8157 - val_loss: 0.4546 - val_acc: 0.8086\n",
      "Epoch 49/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4334 - acc: 0.8159 - val_loss: 0.4604 - val_acc: 0.8104\n",
      "Epoch 50/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4321 - acc: 0.8173 - val_loss: 0.4569 - val_acc: 0.8132\n",
      "Epoch 51/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4290 - acc: 0.8145 - val_loss: 0.4547 - val_acc: 0.8136\n",
      "Epoch 52/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4233 - acc: 0.8189 - val_loss: 0.4627 - val_acc: 0.8104\n",
      "Epoch 53/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4217 - acc: 0.8175 - val_loss: 0.4613 - val_acc: 0.8113\n",
      "Epoch 54/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4215 - acc: 0.8187 - val_loss: 0.4610 - val_acc: 0.8127\n",
      "Epoch 55/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4215 - acc: 0.8191 - val_loss: 0.4568 - val_acc: 0.8099\n",
      "Epoch 56/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4226 - acc: 0.8185 - val_loss: 0.4624 - val_acc: 0.8127\n",
      "Epoch 57/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4203 - acc: 0.8258 - val_loss: 0.4579 - val_acc: 0.8081\n",
      "Epoch 58/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4194 - acc: 0.8191 - val_loss: 0.4594 - val_acc: 0.8086\n",
      "Epoch 59/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4081 - acc: 0.8262 - val_loss: 0.4610 - val_acc: 0.8058\n",
      "Epoch 60/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4086 - acc: 0.8228 - val_loss: 0.4667 - val_acc: 0.8099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4079 - acc: 0.8270 - val_loss: 0.4701 - val_acc: 0.8141\n",
      "Epoch 62/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4102 - acc: 0.8252 - val_loss: 0.4686 - val_acc: 0.8007\n",
      "Epoch 63/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4020 - acc: 0.8287 - val_loss: 0.4714 - val_acc: 0.8003\n",
      "Epoch 64/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4027 - acc: 0.8193 - val_loss: 0.4674 - val_acc: 0.8063\n",
      "Epoch 65/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4020 - acc: 0.8236 - val_loss: 0.4644 - val_acc: 0.8118\n",
      "Epoch 66/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4080 - acc: 0.8270 - val_loss: 0.4657 - val_acc: 0.8104\n",
      "Epoch 67/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.4015 - acc: 0.8289 - val_loss: 0.4648 - val_acc: 0.8113\n",
      "Epoch 68/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.3973 - acc: 0.8242 - val_loss: 0.4661 - val_acc: 0.8058\n",
      "Epoch 69/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.3998 - acc: 0.8323 - val_loss: 0.4590 - val_acc: 0.8072\n",
      "Epoch 70/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.3953 - acc: 0.8319 - val_loss: 0.4597 - val_acc: 0.8099\n",
      "Epoch 71/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.3941 - acc: 0.8266 - val_loss: 0.4692 - val_acc: 0.8067\n",
      "Epoch 72/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.3898 - acc: 0.8343 - val_loss: 0.4644 - val_acc: 0.8058\n",
      "Epoch 73/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.3888 - acc: 0.8303 - val_loss: 0.4770 - val_acc: 0.7966\n",
      "Epoch 74/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.3891 - acc: 0.8345 - val_loss: 0.4671 - val_acc: 0.8053\n",
      "Epoch 75/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.3853 - acc: 0.8380 - val_loss: 0.4668 - val_acc: 0.8040\n",
      "Epoch 76/150\n",
      "5068/5068 [==============================] - 5s 1ms/step - loss: 0.3866 - acc: 0.8374 - val_loss: 0.4699 - val_acc: 0.7980\n",
      "Epoch 77/150\n",
      "4960/5068 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8403"
     ]
    }
   ],
   "source": [
    "model = LeNetPlus()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=150,\n",
    "          verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-43-acc0b8ab4abf>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-43-acc0b8ab4abf>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def AlexNet()\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def AlexNet():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(197,197,3)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # model.add(ZeroPadding2D((1,1)))\n",
    "    # model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    # model.add(ZeroPadding2D((1,1)))\n",
    "    # model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    # model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # model.add(ZeroPadding2D((1,1)))\n",
    "    # model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # model.add(ZeroPadding2D((1,1)))\n",
    "    # model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # model.add(ZeroPadding2D((1,1)))\n",
    "    # model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # model.add(ZeroPadding2D((1,1)))\n",
    "    # model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # model.add(ZeroPadding2D((1,1)))\n",
    "    # model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # model.add(ZeroPadding2D((1,1)))\n",
    "    # model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # model.add(ZeroPadding2D((1,1)))\n",
    "    # model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # model.add(ZeroPadding2D((1,1)))\n",
    "    # model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    # model.add(Dense(1024, activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5068 samples, validate on 2173 samples\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, epochs=2,\n",
    "          verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
